{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model3.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"70DLrCO5U4WG"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"egyt6LGFWXmJ"},"source":["# importing modules"]},{"cell_type":"code","metadata":{"id":"2qBo9F6iWaQP"},"source":["import numpy as np\n","import pandas as pd\n","import random\n","from gensim.models import word2vec\n","from gensim.models import KeyedVectors\n","from itertools import chain \n","from collections import Counter \n","\n","file_path = '/content/drive/My Drive/My Projects/FYP/Sentiment Lexicon/Implementation/Model3 - Lankadeepa News Comments/'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ipi_ObyWa-Mv"},"source":["# 1. remove punctuation marks, numbers, foreign words and special characters"]},{"cell_type":"code","metadata":{"id":"5Lhj8p15znFn","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598171976681,"user_tz":-330,"elapsed":61885,"user":{"displayName":"Binod Karunanayake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFmCwCRvBzOjaBqG4sPvRjFr4hASprgBsu6WK6jg=s64","userId":"08880854072199469127"}},"outputId":"4a209fd1-28db-436e-f74f-62595ddfe097"},"source":["comments_df = pd.read_csv(file_path + '0_comments_all.csv', header=0, delimiter=';', quoting=3)\n","comments = comments_df['comment']\n","words_list = []\n","for comment in comments:\n","  words = str(comment).strip().split()\n","  for word in words:\n","    new_word = ''\n","    for character in word:\n","      if 3456 <= ord(character) and ord(character) <= 3583:\n","        continue\n","        new_word += character\n","      else:\n","        if len(new_word) > 0:\n","          words_list.append(new_word)\n","        new_word = ''\n","    else:\n","    if len(new_word) > 0:\n","      words_list.append(new_word)\n","\n","noise_removed_words_df = pd.DataFrame(words_list, columns=['word'])\n","noise_removed_words_df.to_csv(file_path + '1_removed_noise.csv', sep=',', encoding='utf-8', index=False)\n","print('created 1_removed_noise.csv with %i words from %i comments' % (noise_removed_words_df.shape[0], comments_df.shape[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["created 1_removed_noise.csv with 4680274 words from 290189 comments\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9Ktwj5dnfUSd"},"source":["# 2. remove duplicates and find frequencies"]},{"cell_type":"code","metadata":{"id":"tmJpx_TyW12I","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598171987653,"user_tz":-330,"elapsed":7511,"user":{"displayName":"Binod Karunanayake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFmCwCRvBzOjaBqG4sPvRjFr4hASprgBsu6WK6jg=s64","userId":"08880854072199469127"}},"outputId":"4ec5e48b-425c-4e92-d57b-1f2c6422a5f5"},"source":["frequencies = {}\n","\n","for word in words_list:\n","  if word not in frequencies.keys():\n","    frequencies[word] = 1\n","  else:\n","    frequencies[word] += 1\n","\n","fo = open(file_path +'untagged.txt', 'w')\n","for word in frequencies.keys():\n","  fo.write(word + '\\n')\n","fo.close()\n","\n","unique_words_df = pd.DataFrame(frequencies.items(), columns=['word', 'frequency'])\n","unique_words_df.to_csv(file_path + '2_removed_duplicates.csv', sep=',', encoding='utf-8', index=False)\n","\n","print('created 2_removed_duplicates.csv with %i unique words from %i words' % (len(frequencies), len(words_list)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["created 2_removed_duplicates.csv with 223338 unique words from 4680274 words\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"furUBr4Au9AP"},"source":["# 3. filter words by POS tag (adjectives, adverbs)"]},{"cell_type":"code","metadata":{"id":"IfwEwxTlZqlD","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598172115851,"user_tz":-330,"elapsed":1325,"user":{"displayName":"Binod Karunanayake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFmCwCRvBzOjaBqG4sPvRjFr4hASprgBsu6WK6jg=s64","userId":"08880854072199469127"}},"outputId":"c3558373-42b9-4fd6-e274-bc37433b22b4"},"source":["# JJ  Adjective\n","# JCV Adjective in Compound Verbs\n","# RB  Adverb\n","# NIP Nipathana\n","# AUX Modal Auxiliary\n","\n","fo = open(file_path + 'tagged.txt', 'r', encoding='utf-8')\n","lines = fo.readlines()\n","fo.close()\n","\n","pos_tags = {}\n","for line in lines:\n","  if line.startswith('%%'):\n","    pass\n","  else:\n","    word, pos_tag = line.strip().split()\n","    if pos_tag in ['JJ', 'JCV', 'NIP', 'AUX', 'RB']:\n","      pos_tags[word] = pos_tag\n","\n","filtered_pos_tagged_words_df = pd.DataFrame(pos_tags.items(), columns=['word', 'pos_tag'])\n","filtered_pos_tagged_words_df = unique_words_df.merge(filtered_pos_tagged_words_df, how='inner', on='word')\n","filtered_pos_tagged_words_df = filtered_pos_tagged_words_df.sort_values('word')\n","filtered_pos_tagged_words_df.to_csv(file_path + '3_filtered_by_pos_tag.csv', sep=',', encoding='utf-8', index=False)\n","\n","print('created 3_filtered_by_pos_tag.csv with %i words' % len(filtered_pos_tagged_words_df))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["created 3_filtered_by_pos_tag.csv with 9257 words\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TljqDwCET8KO"},"source":["fo = open(file_path + 'tagged.txt', 'r', encoding='utf-8')\n","lines = fo.readlines()\n","fo.close()\n","\n","fo = open(file_path + 'lankadeepa_pos_tagged_words.csv', 'w', encoding='utf-8')\n","fo.write('word,pos_tag\\n')\n","for line in lines:\n","  if line.startswith('%%'):\n","    pass\n","  else:\n","    word, pos_tag = line.strip().split()\n","    fo.write(word + ',' + pos_tag + '\\n')\n","fo.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SYoeTZrUUR-V"},"source":["df = pd.read_csv(file_path + 'lankadeepa_pos_tagged_words.csv', header=0)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2dAI6nEML0_"},"source":["# 4. filter words by synset"]},{"cell_type":"code","metadata":{"id":"CuU97wf7sxho","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598017322203,"user_tz":-330,"elapsed":1757,"user":{"displayName":"Binod Karunanayake","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgFmCwCRvBzOjaBqG4sPvRjFr4hASprgBsu6WK6jg=s64","userId":"08880854072199469127"}},"outputId":"92aeaac7-90dd-4502-c020-26698fa3cd9b"},"source":["words = filtered_pos_tagged_words_df['word']\n","\n","# ==============================================================================\n","# fixed length stem approach \n","# ==============================================================================\n","\n","# n = 5\n","# stem = words[0][:n-1]\n","# group = []\n","# for word in words:\n","#   if stem in word:\n","#     group.append(word)\n","#   else:\n","#     if len(word) > n:\n","#       stem = word[:n-1]\n","#       print(group)\n","#       group = [word]\n","#     else:\n","#       print([word])\n","\n","# ==============================================================================\n","# dynamic length stem approach \n","# ==============================================================================\n","\n","synset_filtered_words = []\n","stem = words.iloc[0]\n","group = []\n","for word in words:\n","  if word.startswith(stem):\n","    group.append(word)\n","  else:\n","    max_word = group[0]\n","    max_frequency = frequencies[group[0]]\n","    for item in group:\n","      if frequencies[item] > max_frequency:\n","        max_word = item\n","        max_frequency = frequencies[item]\n","    synset_filtered_words.append(max_word)\n","    stem = word\n","    group = [word]\n","\n","filtered_synset_words_df = pd.DataFrame(synset_filtered_words, columns=['word'])\n","filtered_synset_words_df = filtered_pos_tagged_words_df.merge(filtered_synset_words_df, how='inner', on='word')\n","filtered_synset_words_df = filtered_synset_words_df.sort_values('frequency', ascending=False)\n","filtered_synset_words_df.to_csv(file_path + '4_filtered_by_synset.csv', sep=',', encoding='utf-8', index=False)\n","\n","print('created 4_filtered_by_synset.csv with %i words' % len(filtered_synset_words_df))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["created 4_filtered_by_synset.csv with 5841 words\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xAhcyWvupZO0"},"source":["# 5. filter words by average frequency"]},{"cell_type":"code","metadata":{"id":"WK-z6fxtpZez"},"source":["total = 0\n","for word in frequencies.keys():\n","  total += frequencies[word]\n","average_frequency = total / len(frequencies.keys())\n","print(average_frequency, len(frequencies.keys()))\n","\n","words = filtered_synset_words_df['word']\n","frequency_filtered_words = []\n","for word in words:\n","  if frequencies[word] > average_frequency:\n","    frequency_filtered_words.append(word)\n","\n","filtered_frequency_words_df = pd.DataFrame(frequency_filtered_words, columns=['word'])\n","filtered_frequency_words_df = filtered_synset_words_df.merge(filtered_frequency_words_df, how='inner', on='word')\n","filtered_frequency_words_df = filtered_frequency_words_df.sort_values('word', ascending=True)\n","filtered_frequency_words_df.to_csv(file_path + '5_filtered_by_frequency.csv', sep=',', encoding='utf-8', index=False)\n","\n","print('created 5_filtered_by_frequency.csv with %i words' % len(filtered_frequency_words_df))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cmR751CXdFyt"},"source":["# 6. calculate Cohen's kappa value for final data set\n","\n"]},{"cell_type":"code","metadata":{"id":"ZK8PVRpHCwZG"},"source":["import gspread\n","from oauth2client.client import GoogleCredentials\n","from google.colab import auth\n","auth.authenticate_user()\n","gc = gspread.authorize(GoogleCredentials.get_application_default())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcQwhpwUCyNs"},"source":["spreadsheet = gc.open('5_filtered_by_frequency')\n","worksheet = spreadsheet.worksheet('Sheet1')\n","words = worksheet.col_values(1)[1:]\n","binod = worksheet.col_values(2)[1:]\n","udyogi = worksheet.col_values(3)[1:]\n","frequency = worksheet.col_values(4)[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgn-lsGlEZlf"},"source":["p = 0\n","n = 0\n","for sentiment in udyogi:\n","  if sentiment == '1':\n","    p += 1\n","  elif sentiment == '-1':\n","    n += 1\n","print(\"Total sentiment tagged words -\", p + n)\n","print(\"Positive words -\", p)\n","print(\"Negative words -\", n)\n","\n","y1 = binod\n","y2 = udyogi\n","from sklearn.metrics import cohen_kappa_score\n","kappa = cohen_kappa_score(y1, y2, labels=None, weights=None)\n","print(\"Cohen's Kappa Score -\", kappa)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aRhzFzKtVD6s"},"source":["# 7. model"]},{"cell_type":"code","metadata":{"id":"thpBUBCXVNxG"},"source":["################################################################################\n","# Loading word embedding models\n","################################################################################\n","from gensim.models import word2vec\n","from gensim.models import FastText\n","word2vec_model = word2vec.Word2Vec.load('/content/drive/My Drive/My Projects/FYP/Sentiment Lexicon/Implementation/Model1 - Word Embeddings/word2vec_model3/word2vec.model')\n","fasttext_model = FastText.load('/content/drive/My Drive/My Projects/FYP/Sentiment Lexicon/Implementation/Model1 - Word Embeddings/fasttext_model3/fasttext.model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWYDqg8xm7HG"},"source":["################################################################################\n","# Model\n","################################################################################\n","def sentiment_polarity(word, seed_set, model, threshold_polarity=0, threshold_similarity=0):\n","  # find score\n","  n = len(seed_set)\n","  total_similarity = 0\n","  for seed_word in seed_set.keys():\n","    similarity = 0\n","    if model == 'word2vec':\n","      similarity = seed_set[seed_word] * word2vec_model.similarity(word, seed_word)\n","    if model == 'fasttext':\n","      similarity = seed_set[seed_word] * fasttext_model.similarity(word, seed_word)\n","    if abs(similarity) > threshold_similarity:\n","      total_similarity += similarity\n","  score = total_similarity / n\n","\n","  # find polarity\n","  polarity = 1 if score >= threshold_polarity else -1\n","\n","  return polarity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k0GFUCmcyHVh"},"source":["################################################################################\n","# Performance\n","################################################################################\n","from sklearn import metrics\n","def evaluate(y_true, y_pred):\n","  y_true_list = y_true\n","  y_pred_list = y_pred\n","\n","  classification_report = metrics.classification_report(y_true_list, y_pred_list, digits=4, output_dict=True)\n","  results = classification_report['macro avg']\n","  results['accuracy'] = classification_report['accuracy']\n","\n","  return results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l36DYoShdM0y"},"source":["# 8. evaluating test set"]},{"cell_type":"code","metadata":{"id":"NsUmmoljR2Nz"},"source":["import gspread\n","from oauth2client.client import GoogleCredentials\n","from google.colab import auth\n","auth.authenticate_user()\n","gc = gspread.authorize(GoogleCredentials.get_application_default())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCjjSe6j_OKK"},"source":["spreadsheet = gc.open('6_seed_words')\n","worksheet = spreadsheet.worksheet('6_seed_words')\n","words = worksheet.col_values(1)[1:]\n","scores = worksheet.col_values(2)[1:]\n","frequencies = worksheet.col_values(3)[1:]\n","\n","positive_words = []\n","negative_words = []\n","\n","for i in range(len(words)):\n","  if scores[i] == '1':\n","    positive_words.append(words[i])\n","  else:\n","    negative_words.append(words[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"thtruTB0_Iv7"},"source":["spreadsheet = gc.open('7_test_set')\n","# worksheet = spreadsheet.worksheet('7_test_set')\n","worksheet = spreadsheet.worksheet('test_set_revised')\n","test_words = worksheet.col_values(1)\n","test_scores = worksheet.col_values(2)\n","\n","test_set = {}\n","for i in range(len(test_words)):\n","  test_set[test_words[i]] = int(test_scores[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkEMO6YN93Ua"},"source":["Y1_acc = []\n","Y1_pre = []\n","Y1_rec = []\n","Y1_f1s = []\n","\n","for size in range (2, 400, 2):\n","  seed_set = {}\n","  for i in range(int(size/2)):\n","    seed_set[positive_words[i]] = 1\n","    seed_set[negative_words[i]] = -1\n","  y_true = []\n","  y_pred = []\n","  for test_word in test_set.keys():\n","    y_true.append(test_set[test_word])\n","    y_pred.append(sentiment_polarity(test_word, seed_set, 'word2vec'))\n","  results = evaluate(y_true, y_pred)\n","  Y1_acc.append(results['accuracy'])\n","  Y1_pre.append(results['precision'])\n","  Y1_rec.append(results['recall'])\n","  Y1_f1s.append(results['f1-score'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YQqPHI-UuRVM"},"source":["Y2_acc = []\n","Y2_pre = []\n","Y2_rec = []\n","Y2_f1s = []\n","\n","for size in range (356, 358, 2):\n","  seed_set = {}\n","  for i in range(int(size/2)):\n","    seed_set[positive_words[i]] = 1\n","    seed_set[negative_words[i]] = -1\n","  y_true = []\n","  y_pred = []\n","  for test_word in test_set.keys():\n","    y_true.append(test_set[test_word])\n","    y_pred.append(sentiment_polarity(test_word, seed_set, 'fasttext'))\n","  results = evaluate(y_true, y_pred)\n","  Y2_acc.append(results['accuracy'])\n","  Y2_pre.append(results['precision'])\n","  Y2_rec.append(results['recall'])\n","  Y2_f1s.append(results['f1-score'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yoqxj2NDyoj2"},"source":["import matplotlib.pyplot as plt\n","\n","X = range(2, 400, 2)\n","Y1 = Y1_f1s\n","Y2 = Y2_f1s\n","# plt.plot(X, Y1, color='r', label='Word2Vec')\n","plt.plot(X, Y2, color='b', label='FastText')\n","plt.plot([0, 400], [max(Y2_f1s)+0.005, max(Y2_f1s)+0.005], color='r', linestyle='-', linewidth=1)\n","plt.title('No of seed words vs F1-Score')\n","plt.xlabel('no of seed words')\n","plt.ylabel('f1-score')\n","plt.axis([0, 400, 0.5, 1])\n","plt.legend(loc=\"upper right\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Grwn_ZCt5QnB"},"source":["# max(Y2_f1s)\n","# Y2_f1s.index(max(Y2_f1s))\n","print(Y2_acc)\n","print(Y2_pre)\n","print(Y2_rec)\n","print(Y2_f1s)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DTE92qdVvrbT"},"source":["seed_set = {}\n","for i in range(74):\n","  seed_set[positive_words[i]] = 1\n","  seed_set[negative_words[i]] = -1\n","\n","thresholds_polarity = np.arange(-0.002, 0.003, 0.001)\n","thresholds_similarity = np.arange(0.00, 0.02, 0.01)\n","\n","max_x = 0\n","max_y = 0\n","max_z = 0\n","K = {}\n","rows = []\n","for threshold_polarity in thresholds_polarity:\n","  print(threshold_polarity)\n","  row = []\n","  for threshold_similarity in thresholds_similarity:\n","    # print(threshold_similarity)\n","    y_true = []\n","    y_pred = []\n","    for test_word in test_set.keys():\n","      y_true.append(test_set[test_word])\n","      y_pred.append(sentiment_polarity(test_word, seed_set, 'fasttext', threshold_polarity, threshold_similarity))\n","    results = evaluate(y_true, y_pred)\n","    row.append(results['f1-score'])\n","    if max_z < results['f1-score']:\n","      max_x = threshold_polarity\n","      max_y = threshold_similarity\n","      max_z = results['f1-score']\n","      K['accuracy'] = results['accuracy']\n","      K['precision'] = results['precision']\n","      K['recall'] = results['recall']\n","      K['f1-score'] = results['f1-score']\n","  rows.append(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KJtIdI7_vViu"},"source":["import matplotlib.pyplot as plt\n","\n","X, Y = np.meshgrid(thresholds_polarity, thresholds_similarity)\n","Z = np.array(rows).T\n","fig, ax = plt.subplots(1,1)\n","cp = ax.contourf(X, Y, Z, 16)\n","# ax.plot([max_x], [max_y], 'ro')\n","fig.colorbar(cp) # Add a colorbar to a plot\n","ax.set_title('F1-Score (fasttext)')\n","ax.set_xlabel('thresholds_polarity')\n","ax.set_ylabel('threshold_similarity')\n","plt.show()\n","print('(%f, %f)' % (max_x, max_y))\n","print('accuracy: %f\\nprecision: %f\\nrecall: %f\\nf1-score: %f' % (K['accuracy'], K['precision'], K['recall'], K['f1-score']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9M4hUvIpBzgo"},"source":["# 9. create final lexicon"]},{"cell_type":"code","metadata":{"id":"V4UUXuYC_pBK"},"source":["import gspread\n","from oauth2client.client import GoogleCredentials\n","from google.colab import auth\n","auth.authenticate_user()\n","gc = gspread.authorize(GoogleCredentials.get_application_default())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rhI5q0O_rwQ"},"source":["spreadsheet = gc.open('7_test_set')\n","# worksheet = spreadsheet.worksheet('Sheet1')\n","worksheet = spreadsheet.worksheet('test_set_revised')\n","senti_words = worksheet.col_values(1)[0:]\n","senti_scores = worksheet.col_values(2)[0:]\n","worksheet = spreadsheet.worksheet('Sheet2')\n","all_words = worksheet.col_values(1)[0:]\n","all_scores = worksheet.col_values(2)[0:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z33HwBmi_7R6"},"source":["lexicon = {}\n","for i in range(len(senti_words)):\n","  stem = senti_words[i]\n","  for word in all_words:\n","    if word.startswith(stem):\n","      lexicon[word] = senti_scores[i]\n","\n","fo = open(file_path + 'lexicon.csv', 'w', encoding='utf-8')\n","for word in lexicon.keys():\n","  fo.write(word + ',' + lexicon[word] + '\\n')\n","fo.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hFGdLs--0CQM"},"source":["spreadsheet = gc.open('7_test_set')\n","worksheet = spreadsheet.worksheet('test_set_revised')\n","senti_words = worksheet.col_values(1)[0:]\n","senti_scores = worksheet.col_values(2)[0:]\n","\n","fo = open(file_path + '3_filtered_by_pos_tag.csv', 'r', encoding='utf-8')\n","lines = fo.readlines()\n","fo.close()\n","\n","words = {}\n","for line in lines[1:]:\n","  word, frequency, pos_tag = line.strip().split(',')\n","  words[word] = int(frequency)\n","\n","for senti_word in senti_words:\n","  print(words[senti_word])    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7t8kmxnSxPE"},"source":["# 10. sentiment analysis using our lexicon"]},{"cell_type":"code","metadata":{"id":"kI3uZX7sS4jv"},"source":["################################################################################\n","# Loading seed set\n","################################################################################\n","fo = open(file_path + '10_lexicon.csv', 'r', encoding='utf-8')\n","lines = fo.readlines()\n","fo.close()\n","\n","seed_set = {}\n","for line in lines:\n","  word, score = line.strip().split(',')\n","  seed_set[word] = int(score)\n","\n","################################################################################\n","# Testing\n","################################################################################\n","fo = open(file_path + '9_tagged_comments.csv', 'r', encoding='utf-8')\n","lines = fo.readlines()\n","fo.close()\n","\n","y_true = []\n","y_pred = []\n","for line in lines:\n","  docid, comment, tag = line.strip().split(';')\n","  true_score = 0\n","  if tag == 'POSITIVE':\n","    true_score = 1\n","  elif tag == 'NEGATIVE':\n","    true_score = -1\n","  words = comment.split()\n","  total_score = 0\n","  for word in words:\n","    if word in seed_set.keys():\n","      total_score += seed_set[word]\n","  pred_score = 0\n","  if total_score > 0:\n","    pred_score = 1\n","  else:\n","    pred_score = -1\n","  # print(true_score, pred_score)\n","  y_true.append(true_score)\n","  y_pred.append(pred_score)\n","\n","################################################################################\n","# Performance\n","################################################################################\n","from tabulate import tabulate\n","from sklearn import metrics\n","\n","y_true_list = y_true\n","y_pred_list = y_pred\n","\n","classification_report = metrics.classification_report(y_true_list, y_pred_list, digits=4, output_dict=True)\n","results = classification_report['macro avg']\n","results['accuracy'] = classification_report['accuracy']\n","\n","rows = [\n","  ['Accuracy', results['accuracy']],\n","  ['Precision', results['precision']],\n","  ['Recall', results['recall']],\n","  ['F1-Score', results['f1-score']]\n","]\n","\n","print(tabulate(rows, tablefmt='github'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIrRCcGtT0n6"},"source":["################################################################################\n","# Loading seed set\n","################################################################################\n","fo = open(file_path + '10_lexicon.csv', 'r', encoding='utf-8')\n","lines = fo.readlines()\n","fo.close()\n","\n","seed_set = {}\n","for line in lines[1:]:\n","  word, score = line.strip().split(',')\n","  seed_set[word] = int(score)\n","\n","################################################################################\n","# Creating training/test sets\n","################################################################################\n","from sklearn.model_selection import train_test_split\n","\n","fo = open(file_path + '9_tagged_comments.csv', 'r', encoding='utf-8')\n","lines = fo.readlines()\n","fo.close()\n","\n","vectors = []\n","scores = []\n","for line in lines[1:]:\n","  docid, comment, tag = line.strip().split(';')\n","  score = 0\n","  if tag == 'POSITIVE':\n","    score = 1\n","  elif tag == 'NEGATIVE':\n","    score = -1\n","  words = comment.split()\n","  # vector = []\n","  # for word in seed_set.keys():\n","  #   vector.append(seed_set[word] * words.count(word))\n","  p = 0\n","  n = 0\n","  for word in words:\n","    if word in seed_set.keys():\n","      s = seed_set[word]\n","      if s > 0:\n","        p += s\n","      else:\n","        n += s\n","  vector = [p, n]\n","  vectors.append(vector)\n","  scores.append(score)\n","\n","# X_train, X_test, y_train, y_test = train_test_split(vectors, scores, test_size=0.2, random_state=42)\n","################################################################################\n","# Train classifiers\n","################################################################################\n","# from sklearn import svm\n","# clf = svm.SVC()\n","# clf.fit(X_train, y_train)\n","# y_pred = clf.predict(X_test)\n","\n","# from sklearn.naive_bayes import GaussianNB\n","# gnb = GaussianNB()\n","# y_pred = gnb.fit(X_train, y_train).predict(X_test)\n","\n","# from sklearn.neighbors import KNeighborsClassifier\n","# neigh = KNeighborsClassifier(n_neighbors=3)\n","# neigh.fit(X_train, y_train)\n","# y_pred = neigh.predict(X_test)\n","\n","# from sklearn import tree\n","# clf = tree.DecisionTreeClassifier()\n","# clf = clf.fit(X_train, y_train)\n","# y_pred = clf.predict(X_test)\n","\n","# from sklearn.linear_model import SGDClassifier\n","# clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n","# clf.fit(X_train, y_train)\n","# y_pred = clf.predict(X_test)\n","\n","# from sklearn.ensemble import AdaBoostClassifier\n","# clf = AdaBoostClassifier(n_estimators=100)\n","# clf = clf.fit(X_train, y_train)\n","# y_pred = clf.predict(X_test)\n","\n","from sklearn.model_selection import cross_validate\n","from sklearn import svm\n","clf = svm.SVC(kernel='linear', C=1)\n","scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n","scores = cross_validate(clf, vectors, scores, cv=5, scoring=scoring)\n","\n","# from sklearn.model_selection import cross_validate\n","# from sklearn.naive_bayes import GaussianNB\n","# gnb = GaussianNB()\n","# scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n","# scores = cross_validate(gnb, vectors, scores, cv=5, scoring=scoring)\n","\n","# from sklearn.model_selection import cross_validate\n","# from sklearn import tree\n","# clf = tree.DecisionTreeClassifier()\n","# scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n","# scores = cross_validate(clf, vectors, scores, cv=5, scoring=scoring)\n","\n","print(scores)\n","print('Accuracy -', sum(scores['test_accuracy'])/5)\n","print('Precision -', sum(scores['test_precision_macro'])/5)\n","print('Recall -', sum(scores['test_recall_macro'])/5)\n","print('F1-Score -', sum(scores['test_f1_macro'])/5)\n","\n","################################################################################\n","# Performance\n","################################################################################\n","# from tabulate import tabulate\n","# from sklearn import metrics\n","\n","# y_true_list = y_test\n","# y_pred_list = y_pred\n","\n","# classification_report = metrics.classification_report(y_true_list, y_pred_list, digits=4, output_dict=True)\n","# results = classification_report['macro avg']\n","# results['accuracy'] = classification_report['accuracy']\n","\n","# rows = [\n","#   ['Accuracy', results['accuracy']],\n","#   ['Precision', results['precision']],\n","#   ['Recall', results['recall']],\n","#   ['F1-Score', results['f1-score']]\n","# ]\n","\n","# print(tabulate(rows, tablefmt='github'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAtNWHxx_pT2"},"source":["################################################################################\n","# Loading seed set\n","################################################################################\n","fo = open(file_path + '10_lexicon.csv', 'r', encoding='utf-8')\n","lines = fo.readlines()\n","fo.close()\n","\n","seed_set = {}\n","for line in lines[1:]:\n","  word, score = line.strip().split(',')\n","  seed_set[word] = int(score)\n","\n","################################################################################\n","# Creating training/test sets\n","################################################################################\n","fo = open(file_path + '9_tagged_comments.csv', 'r', encoding='utf-8')\n","lines = fo.readlines()\n","fo.close()\n","\n","bow = {}\n","i = 1\n","doc_list = []\n","tag_list = []\n","for line in lines[1:]:\n","  docid, comment, tag = line.strip().split(';')\n","  score = 0\n","  if tag == 'POSITIVE':\n","    score = 1\n","  elif tag == 'NEGATIVE':\n","    score = -1\n","  words = comment.split()\n","  for word in words:\n","    if word not in bow.keys():\n","      bow[word] = i\n","      i += 1\n","  doc_list.append(comment)\n","  tag_list.append(score)\n","\n","vec_list = []\n","for doc in doc_list:\n","  vec = []\n","  words = set(doc.split())\n","  for word in words:\n","    if word in seed_set.keys():\n","      vec.append(bow[word]/20000)\n","      vec.append(doc.count(word))\n","      vec.append(seed_set[word])\n","      # vec.append(0)\n","    else:\n","      vec.append(bow[word]/20000)\n","      vec.append(doc.count(word))\n","      vec.append(0)\n","  vec += [0] * (390 - len(vec))\n","  vec_list.append(np.array(vec))\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(vec_list, tag_list, test_size=0.2, random_state=10)\n","# X_train = np.array(X_train).reshape((4008, 130))\n","# X_test = np.array(X_test).reshape((1002, 130))\n","################################################################################\n","# Train classifiers\n","################################################################################\n","from sklearn import svm\n","clf = svm.SVC()\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","\n","# from sklearn.naive_bayes import GaussianNB\n","# gnb = GaussianNB()\n","# y_pred = gnb.fit(X_train, y_train).predict(X_test)\n","\n","# from sklearn import tree\n","# clf = tree.DecisionTreeClassifier()\n","# clf = clf.fit(X_train, y_train)\n","# y_pred = clf.predict(X_test)\n","\n","# from sklearn.neighbors import KNeighborsClassifier\n","# neigh = KNeighborsClassifier(n_neighbors=3)\n","# neigh.fit(X_train, y_train)\n","# y_pred = neigh.predict(X_test)\n","\n","################################################################################\n","# Performance\n","################################################################################\n","from tabulate import tabulate\n","from sklearn import metrics\n","\n","y_true_list = y_test\n","y_pred_list = y_pred\n","\n","classification_report = metrics.classification_report(y_true_list, y_pred_list, digits=4, output_dict=True)\n","results = classification_report['macro avg']\n","results['accuracy'] = classification_report['accuracy']\n","\n","rows = [\n","  ['Accuracy', results['accuracy']],\n","  ['Precision', results['precision']],\n","  ['Recall', results['recall']],\n","  ['F1-Score', results['f1-score']]\n","]\n","\n","print(tabulate(rows, tablefmt='github'))\n","\n","################################################################################\n","# Cross validation\n","################################################################################\n","# from sklearn.model_selection import cross_validate\n","# from sklearn import svm\n","# clf = svm.SVC(kernel='linear', C=1)\n","# scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n","# scores = cross_validate(clf, vec_list, tag_list, cv=5, scoring=scoring)\n","\n","# from sklearn.model_selection import cross_validate\n","# from sklearn.naive_bayes import GaussianNB\n","# gnb = GaussianNB()\n","# scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n","# scores = cross_validate(gnb, vectors, scores, cv=5, scoring=scoring)\n","\n","# from sklearn.model_selection import cross_validate\n","# from sklearn import tree\n","# clf = tree.DecisionTreeClassifier()\n","# scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n","# scores = cross_validate(clf, vectors, scores, cv=5, scoring=scoring)\n","\n","# print(scores)\n","# print('Accuracy -', sum(scores['test_accuracy'])/5)\n","# print('Precision -', sum(scores['test_precision_macro'])/5)\n","# print('Recall -', sum(scores['test_recall_macro'])/5)\n","# print('F1-Score -', sum(scores['test_f1_macro'])/5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CN2v0W1sIMJ"},"source":["print(np.array(X_train).shape)\n","a = np.array(X_train).reshape((4008,260))"],"execution_count":null,"outputs":[]}]}